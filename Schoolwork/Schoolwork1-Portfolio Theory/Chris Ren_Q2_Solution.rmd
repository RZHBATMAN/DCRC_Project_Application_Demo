---
title: "Chris Ren_Q2_Solution"
output:
  html_document:
    df_print: paged
  pdf_document: default
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```
# Q2


```{r message=FALSE}

performance_df = read.csv('../datasets/performance.csv')
info_df = read.delim('../datasets/info.txt',sep=' ')
tail(performance_df)
head(info_df)

```

## (a)
Now I use sqldf to merge the dataframes.
```{r message=FALSE}
library(sqldf)
join_string =  "select
                info_df.Name,performance_df.Date,performance_df.performance
                from info_df
                left join performance_df
                on performance_df.ID = info_df.ID"
join_df = sqldf(join_string,stringsAsFactors = FALSE)
head(join_df,10)
```

## (b)
Now I use the openxlsx to write the file.
```{r message=FALSE}
library(openxlsx)
wb=createWorkbook()
addWorksheet(wb,'joined_dataframe')
writeData(wb,'joined_dataframe',join_df)
saveWorkbook(wb,file='Q2_joined_dataframe.xlsx',overwrite=TRUE)

```

## (c)
Now we implement OLS regression with intercept. Denote $X = [\vec{1},\vec{x}]$ where $x$ is the performance of Index. Denote $y$ as the performance of Stock CZ. By minimizing the sum of squared residuals, we can have a close-form estimator on the coefficient $\vec{\theta}=[\hat{\alpha},\hat{\beta}]^T=(X^TX)^{-1}X^Ty$ where $\alpha$ represents the intercept and $\beta$ represents the coefficient before Index. For convenience, we will just use the built-in function for regression in R.
```{r message=FALSE}
reg_df=merge(x=join_df[join_df$Name=="Stock CZ",],join_df[join_df$Name=="Index",],by='Date',all.x=TRUE)[,c(1,3,5)]
colnames(reg_df)=c("Date","CZ","Index")
head(reg_df)
OLS=lm(CZ~Index,data=reg_df)
summary(OLS)
```
We are able to see that $\hat{\alpha}=-0.00028, \hat{\beta}=0.8069$. p-value for $\alpha$ is too large, suggesting that we do not reject the hypothesis $\alpha=0$, so $\alpha$ is not siginificant. p-value for $\beta$ is very small, suggesting that $\beta$ is significant. 

Futhermore, as OLS assumes error inpersistence (no autocorrelation) and exogeneity (error is uncorrelated with features) and homoskedasticity (errors are i.i.d. distributed), we also need to examine the pattern within residuals to make sure our OLS works here. 

```{r message=FALSE}
library(plotly)
resid=resid(OLS)
#plot Autocorrelation function (ACF) and histogram
acf(resid)
plot_ly(x=resid,type='histogram',nbinsx = 50)
#examine exogeneity
sprintf("The correlation between residual and x is %s" , cor(resid,reg_df$Index))

```
From the ACF and histogram, we can tell the error is indeed approximately normally distributed and they are not persistent. We can also conclude it's not autocorrelated with our feature x (Index). Therefore, OLS here is robust and consistent. 

## (d)
```{r message=FALSE}
fit=fitted(OLS)
fig=plot_ly(x = fit, y = reg_df$CZ, type = 'scatter',
             mode = "markers", marker = list(color = "pink"))
fig=fig %>% layout(
    title = "Actual vs Fitted",
    xaxis = list(title = "Fitted Performance of Stock CZ"),
    yaxis = list(title = "Actual Performance of Stock CZ"),
    margin = list(l = 100)
  )
fig
```
The plot is consistent with the fact that this model has $R^2=0.93$. It works pretty well, and OLS assumptions are satisfied as I have tested in (c). Great!
